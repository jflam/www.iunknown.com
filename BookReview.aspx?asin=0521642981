
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" >
<HTML>
  <HEAD>
    <title>Recommended book: Information Theory, Inference & Learning Algorithms
    </title>
    <meta content="JavaScript" name="vs_defaultClientScript">
    <meta content="http://schemas.microsoft.com/intellisense/ie5" name="vs_targetSchema">
    <LINK href="global.css" type="text/css" rel="stylesheet">
  </HEAD>
  <body MS_POSITIONING="GridLayout">
    <a href="Default.aspx">
      <img id="Image2" class="Logo" src="images/logo.png" alt="iunknown.com: building better software, faster" border="0" /></a>
    <form name="Form1" method="post" action="BookReview.aspx?asin=0521642981" id="Form1">
<input type="hidden" name="__VIEWSTATE" value="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" />

      <div class="BookReview"><img id="BookImage" class="BookReviewImage" src="http://images.amazon.com/images/P/0521642981.01._SCMZZZZZZZ_.jpg" alt="" border="0" />
        <span id="BookReviewTitle" class="BookReviewTitle">Information Theory, Inference & Learning Algorithms</span>
        <div class="ItemDetail">By
          <span id="Authors">David J. C. MacKay</span></div>
        <div class="ItemDetail">
          <span class="ItemCaption">List price:</span>
          <span id="ListPrice" class="ListPrice">$55.00</span>
          <span class="ItemCaption">Our price:</span>
          <span id="OurPrice" class="OurPrice">$55.00</span>
        </div>
        <div class="ItemDetail">
          <span class="ItemCaption">Average customer review:</span>
          <span id="AverageReview">4.3</span>
          based on
          <span id="TotalReviews">3</span>
          reviews.
        </div>
        <div class="BuyButton">
          <a href='http://www.amazon.com/exec/obidos/ASIN/0521642981/ref=nosim/iunknowncom-20'>
            <img id="Image1" src="Images/buy_from_amazon.gif" alt="" border="0" /></a>
        </div>
        <table class="ReviewBody">
          <tr>
            <td class="ReviewColumn">
              <table id="EditorialReviewsPanel" class="ReviewSection" cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td>
	
                <DIV class="ReviewHeading">Book Description:</DIV>
                <span id="Label1">Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography.   This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks.   The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast.    Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David MacKay's groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way.   In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.</span>
              
</td></tr></table>
              <table id="CustomerReviewsPanel" class="ReviewSection" cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td>
	
                <DIV class="ReviewHeading">Customer reviews:</DIV>
                
                    <div class="CustomerReviewSummary">Brings theory to life</div>
                    <div>
                      <span class="ItemCaption">Rating: </span>
                      <span>5</span>
                      <span class="ItemCaption">Helpful: </span>
                      <span>7</span>
                      /
                      <span>10</span>
                      votes <span class="ItemCaption">Reviewed: </span>
                      <span>2004-02-28</span>
                    </div>
                    <span>Fantastically good value, this wide-ranging textbook covers elementary information theory, data compression, and coding theory; machine learning, Bayesian inference, Monte Carlo methods; and state of the art error-correcting coding methods, including low-density parity-check codes, turbo codes, and digital fountain codes. Theory and practical examples are covered side by side. Hundreds of exercises are included, many with worked solutions.<P>Three things are distinctive about this book.<BR>First, it emphasizes the connections between information theory and machine learning - for example data compression and Bayesian data modelling are two sides of the same coin.<BR>Second, since 1993, there's been a revolution in communication theory, with classical algebraic codes being superceded by sparse graph codes; this text covers these recent developments in detail.<BR><BR><BR>Third, the whole book is available for free online viewing at <BR>www.inference.phy.cam.ac.uk/mackay/itila/.<P>I use this book in all my teaching! :-)</span>
                  
                    <div class="CustomerReviewSummary">Good book - but few arguments need revision from theorists</div>
                    <div>
                      <span class="ItemCaption">Rating: </span>
                      <span>4</span>
                      <span class="ItemCaption">Helpful: </span>
                      <span>5</span>
                      /
                      <span>9</span>
                      votes <span class="ItemCaption">Reviewed: </span>
                      <span>2004-01-12</span>
                    </div>
                    <span>This review concerns only the coding theory part.<P>If you want to know what's presently going on in the field of coding theory with solid technical foundation, this is the book. The importance of this book is it answers why people have been going into new directions into coding theory and provides good information about LDPC codes, turbo codes and decoding algorithms. People have solved some problems that arise in coding field without going into depths of mathematics. Till early 1990's research in coding was intensely mathematical. People thought the packing problem was the answer to the coding problem. However Mackay answers the conventional thought was wrong when one tries to attain shannon limit. He gives an argument based on GV bound (warning: This argument may not be entirely true).<P>Now the bad part of the book. Mackay bases his entire book on the basis that algebraic codes cannot exceed GV bound. This is wrong. If you look at Madhu Sudan's notes at MIT (The prestigious Nevenlinna award winner), he says random codes are not always the best. Specifically he cites an argument which states AG codes exceed GV bound at a faster pace. So packing problem still has a relevance to coding problem as it could help attain shannon limit at a faster pace than random codes. (Warning: Madhu does not state anything about size of blocks. But my feeling is that AG codes since they exceed GV bound faster than random codes one could achieve shannon limit with comparitively smaller blocks). So still mathematicians could hope to contribute to practical coding theory while enriching mathematics. <P>Inspite of this, the book is a must have for engineers and computer scientists.</span>
                  
                    <div class="CustomerReviewSummary">A reservoir of information - Yet few problems</div>
                    <div>
                      <span class="ItemCaption">Rating: </span>
                      <span>4</span>
                      <span class="ItemCaption">Helpful: </span>
                      <span>3</span>
                      /
                      <span>6</span>
                      votes <span class="ItemCaption">Reviewed: </span>
                      <span>2004-01-12</span>
                    </div>
                    <span>This review concerns only the coding theory part.<P>If you want to know what's presently going on in the field of coding theory with solid technical foundation, this is the book. The importance of this book is it answers why people have been going into new directions into coding theory. People have solved some problems that arise in coding field without going into depths of mathematics. Till early 1990's research in coding was intensely mathematical. People thought the packing problem was the answer to the coding problem. However Mackay answers the conventional thought was wrong. He gives an argument based on GV bound.<P>Now the bad part of the book. Mackay bases his entire book on the basis that algebraic codes cannot exceed GV bound. This is wrong. If you look at Madhu Sudan's notes at MIT (The prestigious Nevenlinna award winner), he says random codes are not always the best. Specifically he cites an argument which states AG codes exceed GV bound at a faster pace. So packing problem still has a relevance to coding problem as it could help attain shannon limit at a faster pace than random codes. (Warning: Madhu does not state anything about size of blocks. But my feeling is that AG codes since they exceed GV bound faster than random codes one could achieve shannon limit with comparitively smaller blocks). So still mathematicians could hope to contribute to practical coding theory while enriching mathematics. <P>Another bad part is the book does not talk too much about new problems such as multi-access channels, broadcast channels, zero error information theory, communication complexity, upcoming challenges and open problems and what has been done in these fields in information theory and so on...what has been done in these. May be some author bright researcher in the area like Mackay could write a book to put a direction to these questions.<P>Inspite of this, the book is a must have for engineers and computer scientists.</span>
                  
              
</td></tr></table>
            </td>
            <td class="ReviewSideBar">
              <div class="ItemCaption">Customers also bought:</div>
              
                  <div class="SimilarItem">
                    <a href='BookReview.aspx?asin=0521813972'>
                      Kernel Methods for Pattern Analysis
                    </a>
                    <div class="SimilarItemRatings">Rating: 0 based on 0 reviews</div>
                  </div>
                
                  <div class="SimilarItem">
                    <a href='BookReview.aspx?asin=0521592712'>
                      Probability Theory : The Logic of Science
                    </a>
                    <div class="SimilarItemRatings">Rating: 5.0 based on 7 reviews</div>
                  </div>
                
                  <div class="SimilarItem">
                    <a href='BookReview.aspx?asin=080186982X'>
                      Algebra of Probable Inference
                    </a>
                    <div class="SimilarItemRatings">Rating: 5.0 based on 3 reviews</div>
                  </div>
                
                  <div class="SimilarItem">
                    <a href='BookReview.aspx?asin=0130125342'>
                      Learning Bayesian Networks
                    </a>
                    <div class="SimilarItemRatings">Rating: 5.0 based on 2 reviews</div>
                  </div>
                
                  <div class="SimilarItem">
                    <a href='BookReview.aspx?asin=0486696847'>
                      Information Theory and Statistics (Dover Books on Mathematics)
                    </a>
                    <div class="SimilarItemRatings">Rating: 4.5 based on 2 reviews</div>
                  </div>
                
              <div class="GoogleBookAd">
                <script type="text/javascript"><!--
                google_ad_client = "pub-8264790134546506";
                google_ad_width = 160;
                google_ad_height = 600;
                google_ad_format = "160x600_as";
                google_ad_channel ="4375045039";
                google_color_border = "CCCCCC";
                google_color_bg = "FFFFFF";
                google_color_link = "000000";
                google_color_url = "666666";
                google_color_text = "333333";
                //--></script>
                <script type="text/javascript" src="http://pagead2.googlesyndication.com/pagead/show_ads.js">
                </script>
              </div>
            </td>
          </tr>
        </table>
      </div>
    </form>
  </body>
</HTML>
